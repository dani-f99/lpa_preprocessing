{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76b3ff9-d778-4b3f-b706-c8da2c7f8cf8",
   "metadata": {},
   "source": [
    "---\n",
    "* Importing python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156d1b0e-9deb-49f9-bc43-2869b10b4cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Python modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "from configparser import ConfigParser\n",
    "from distutils.util import strtobool\n",
    "from datetime import datetime\n",
    "import regex as re\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import ast\n",
    "import pymysql\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "print(\"-> Python modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b68824-996b-4d30-8d18-ac742a8af39c",
   "metadata": {},
   "source": [
    "---\n",
    "* Importing custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e101f6-0d3f-4a66-bdd3-27cefd34e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Custom function imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from scripts.mut_regall import *\n",
    "from scripts.round_up import *\n",
    "from scripts.metadata_cond import *\n",
    "from scripts.get_output import *\n",
    "from scripts.assign_metadata import *\n",
    "\n",
    "print(\"-> Custom function imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0812069-fa1e-47e1-9b34-08402903b076",
   "metadata": {},
   "source": [
    "---\n",
    "* Cheeking if tables_output, tables_raw and tables_processed exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c766594b-5aed-498e-a45e-996e640183fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> tables_output folder already exists. \n",
      "   continuing. \n",
      "\n",
      "-> tables_raw folder already exists. \n",
      "   continuing. \n",
      "\n",
      "-> tables_processed folder already exists. \n",
      "   continuing. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_dirs = [\"tables_output\", \"tables_raw\", \"tables_processed\"]\n",
    "mains_dirs_exists = [os.path.exists(i) for i in main_dirs]\n",
    "\n",
    "for i,j in zip(main_dirs,mains_dirs_exists):\n",
    "    if j == False:\n",
    "        print(\"-> Missing {} folder.\".format(i),\"\\n{} folder created.\".format(i),\"\\n\")\n",
    "        os.mkdir(i)\n",
    "    else:\n",
    "        print(\"-> {} folder already exists.\".format(i),\"\\n   continuing.\".format(i),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7588f8e-824d-431a-86d7-6ee25b2aaa79",
   "metadata": {},
   "source": [
    "---\n",
    "* Reading config file\n",
    "* Configuring the MySQL variables\n",
    "* Cheeking if the raw tables need to be imported\n",
    "* if no -> continuing\n",
    "* if yes -> downloading the missing MySQL tables from the ImmuneDB server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7886e7e1-1c4e-465d-b399-7b314ddd215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> config.ini was read successfully.\n"
     ]
    }
   ],
   "source": [
    "config_file = open(\"config.ini\",\"r\")\n",
    "config_str = config_file.read()\n",
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read_string(config_str)\n",
    "\n",
    "print(\"-> config.ini was read successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ca2d7d-c160-480c-b47f-6d6a31693e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_mysql = [\"mysql_\"+i for i in config[\"mysql\"]]\n",
    "\n",
    "for i,j in zip(catg_mysql,config[\"mysql\"]):\n",
    "    values = i.split(\"_\")\n",
    "    globals()[i] = config[values[0]][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51752f3-9bea-4719-a369-c3828b456de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Talbes already imported, continuing...\n"
     ]
    }
   ],
   "source": [
    "req_raw_paths = [\"tables_raw\\\\{db}\\\\{db}.\".format(db=mysql_db)+i+\".csv\" for i in mysql_tables.split(\",\")]\n",
    "\n",
    "#cheeking if the tables exists\n",
    "if sum([os.path.exists(i) for i in req_raw_paths]) < 3:\n",
    "    print(\"\\nMissing raw data tables. \", \"Connecting to MySQL server via config credentials...\")\n",
    "\n",
    "    # connecting to mysql server\n",
    "    try:\n",
    "        conn = pymysql.connect(host = mysql_host,\n",
    "                               port = int(mysql_port),\n",
    "                               user = mysql_user,\n",
    "                               password = mysql_pass,\n",
    "                               db = mysql_db)\n",
    "        \n",
    "        if isinstance(conn,pymysql.connections.Connection):\n",
    "            conn_status = \"\\nSuccessfully connected to MySQL server.\"\n",
    "            print(conn_status)\n",
    "        else:\n",
    "            conn_status = \"\\nError while connecnting to mysql server, cheek config.ini...\"\n",
    "            raise TypeError(conn_status)\n",
    "        \n",
    "    except:\n",
    "        conn_status = \"\\nError while connecnting to mysql server, cheek config.ini...\"\n",
    "        raise TypeError(conn_status)\n",
    "\n",
    "\n",
    "    # making sure dir exists\n",
    "    raw_path = \"tables_raw\\\\{}\".format(mysql_db)\n",
    "    if path.exists(raw_path) == False:\n",
    "        os.mkdir(raw_path)\n",
    "        print(\"\\nCreated database folder\")\n",
    "    else:\n",
    "        print(\"\\nDatabase folder already exists, continuing...\")\n",
    "    \n",
    "    # importing the missing tables\n",
    "    db_tables = mysql_tables.split(\",\")\n",
    "    len_tables = len(db_tables)\n",
    "    path_csv = raw_path\n",
    "    \n",
    "    num = 1\n",
    "    for i in db_tables:\n",
    "        print(\"\\nImporting missing talbes, this process  may take a while... \\n\")\n",
    "        table_name = \"{db}.{table}\".format(db=mysql_db, table=i)\n",
    "        temp_count = \"\\n ({n}/{len})\".format(n=num, len=len_tables)\n",
    "        \n",
    "        if path.exists(raw_path + \"\\\\{t_name}.csv\".format(t_name=table_name)):\n",
    "            print(table_name+\" already imported.\",temp_count,\"\\n -----------\")\n",
    "        else:\n",
    "            print(\"importing {t_name}\".format(t_name=table_name))\n",
    "            temp_df = pd.read_sql_query(\"SELECT * FROM {t_name};\".format(t_name=table_name), conn)\n",
    "            temp_df.to_csv(raw_path+\"\\\\{t_name}.csv\".format(t_name=table_name, table=i))\n",
    "            print(\"Imported {t_name} succecfully.\".format(t_name=table_name), temp_count ,\"\\n -----------\")\n",
    "        num += 1\n",
    "        \n",
    "    print(\"\\nDone importing database tables.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nTalbes already imported, continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed253d-7bba-432f-9126-46bdd2b40deb",
   "metadata": {},
   "source": [
    "---\n",
    "* Loading the raw tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9dca06d-ac58-4a53-b092-c9c23769dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading raw datasets: 100%|██████████| 3/3 [00:11<00:00,  3.86s/dataset, sample_metadata]\n"
     ]
    }
   ],
   "source": [
    "t_names = mysql_tables.split(\",\")\n",
    "\n",
    "with tqdm(t_names, desc=\"loading raw datasets\",unit=\"dataset\", initial=0) as pbar_loadraw:\n",
    "    for tb in pbar_loadraw:\n",
    "        globals()[tb] = pd.read_csv('tables_raw\\\\{db}\\\\{db}.{table}.csv'.format(db=mysql_db, table=tb)\n",
    "                                    ,index_col=0)\n",
    "        pbar_loadraw.set_postfix_str(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4642c2-a37a-46ef-b40f-2c6635f02f4e",
   "metadata": {},
   "source": [
    "---\n",
    "* Creating tables_processed dir if needed\n",
    "* Creating metadata dataframe if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493818a1-6840-415c-bed6-b2e1fcf80762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tables_processed\\covid_vaccine_new already exists, conitnuing...\n"
     ]
    }
   ],
   "source": [
    "path_processed_dir = \"tables_processed\\\\{}\".format(mysql_db)\n",
    "if os.path.exists(path_processed_dir) == False:\n",
    "    os.mkdir(path_processed_dir)\n",
    "    print(path_processed_dir, \"created.\")\n",
    "else:\n",
    "    print(path_processed_dir, \"already exists, conitnuing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8b0675-0eba-402c-9be4-046fba2294b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating covid_vaccine_new.sample_metadata_df.csv...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_24156\\2775119400.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Non-Spike B']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  metadata_df.loc[metadata_df[\"sample_id\"]==i,j] = metadata_og.loc[(metadata_og[\"sample_id\"]==i)&(metadata_og[\"key\"]==j),\"value\"].values\n",
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_24156\\2775119400.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['1 baseline']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  metadata_df.loc[metadata_df[\"sample_id\"]==i,j] = metadata_og.loc[(metadata_og[\"sample_id\"]==i)&(metadata_og[\"key\"]==j),\"value\"].values\n"
     ]
    }
   ],
   "source": [
    "path_metdadata_df = \"tables_processed\\\\{db}\\\\{db}.sample_metadata_df.csv\".format(db=mysql_db)\n",
    "\n",
    "if os.path.exists(path_metdadata_df):\n",
    "    print(\"{db}.sample_metadata_df.csv already created, continuing...\".format(db=mysql_db))\n",
    "    metadata_df = pd.read_csv(path_metdadata_df, index_col=0)\n",
    "                                                                                 \n",
    "else:\n",
    "    print(\"Creating {db}.sample_metadata_df.csv...\".format(db=mysql_db))\n",
    "   \n",
    "    metadata_keys_og = mysql_metadata_og.split(\",\")\n",
    "    metadata_keys_new =  mysql_metadata_new.split(\",\")\n",
    "    meta_dict = dict(zip(metadata_keys_og, metadata_keys_new))\n",
    "\n",
    "    metadata_og = sample_metadata[sample_metadata[\"key\"].isin(metadata_keys_og)]\n",
    "    metadata_og = metadata_og.replace({\"key\":meta_dict})\n",
    "\n",
    "    sample_ids = np.sort(metadata_og[\"sample_id\"].unique())\n",
    "    metadata_df = pd.DataFrame({\"sample_id\":sample_ids})\n",
    "    metadata_df[metadata_keys_new] = np.nan\n",
    "\n",
    "    for i in sample_ids:\n",
    "        temp_sid = i\n",
    "        for j in metadata_keys_new:\n",
    "            cond_sid = (metadata_og[\"sample_id\"] == i)\n",
    "            cond_key = (metadata_og[\"key\"] == j)\n",
    "            metadata_df.loc[metadata_df[\"sample_id\"]==i,j] = metadata_og.loc[(metadata_og[\"sample_id\"]==i)&(metadata_og[\"key\"]==j),\"value\"].values\n",
    "    metadata_df.to_csv(path_metdadata_df)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9e31e-841c-45db-a76c-ce7975cda021",
   "metadata": {},
   "source": [
    "---\n",
    "* Verifing that all the samples id's from metadata file exists in the clone_stats file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41fd8ef-82ca-461d-8f7a-c90ae4089dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing sample_id values.\n"
     ]
    }
   ],
   "source": [
    "metalist_sids = np.sort(metadata_df.sample_id.unique())\n",
    "clones_sids = np.sort(clone_stats.dropna().sample_id.unique()).astype(\"int\")\n",
    "\n",
    "values_missing = np.setdiff1d(clones_sids, metalist_sids)\n",
    "values_common = np.intersect1d(metalist_sids, clones_sids)\n",
    "\n",
    "if len(values_missing) > 0:\n",
    "    print(\"missing sample_id from metadata file:\",values_missing)\n",
    "    clone_stats = clone_stats[clone_stats[\"sample_id\"].isin(values_common)]\n",
    "    raise TypeError(\"verify metadata sample_id values\") \n",
    "\n",
    "else:\n",
    "    print(\"No missing sample_id values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a5604-9536-403d-bcb3-7167a8c60c11",
   "metadata": {},
   "source": [
    "---\n",
    "* loading clones_merged if exists, if not creating and saving\n",
    "* custom function that extract mutations infromation from the \"mutation\" json in each row\n",
    "* Adding the germline infromation to the clone_stats df\n",
    "* Dropping null sample_id rows (cannot assign metadata for those rows)\n",
    "* converting \"sample_id\" values to int instead of floats\n",
    "* assiging the metadata into the merged table (applying assign_metadata)\n",
    "* renaming id_x to id after merging (left had \"id\" colum while right had \"id\"==\"clone_id\")\n",
    "* reseting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0455af67-0f35-4476-accb-a43d579e2478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating clones_merged.csv\n",
      "clones_merged dataframe created and saved, continuing...\n"
     ]
    }
   ],
   "source": [
    "path_clones_merged = \"tables_processed\\\\{db}\\\\{db}.clones_merged.csv\".format(db=mysql_db)\n",
    "\n",
    "if os.path.exists(path_clones_merged):\n",
    "    clones_merged = pd.read_csv(path_clones_merged)\n",
    "    print(\"clones_merged dataframe exists, loading and continuing...\")\n",
    "\n",
    "else:\n",
    "    print(\"Creating clones_merged.csv\")\n",
    "    clones_merged = clone_stats.merge(right=clones[[\"id\",\"germline\"]],\n",
    "                                      how=\"left\",\n",
    "                                      left_on=\"clone_id\",\n",
    "                                      right_on=\"id\")    \n",
    "    \n",
    "    clones_merged = clones_merged[clones_merged[\"sample_id\"].notnull()]        \n",
    "    clones_merged[list(metadata_df.columns)[1:]] = list(clones_merged[\"sample_id\"].apply(assign_metadata, args=(metadata_df,)))\n",
    "    clones_merged.rename({\"id_x\":\"id\"},axis=\"columns\",inplace=True)\n",
    "    clones_merged.reset_index(drop=True, inplace=True)\n",
    "    clones_merged.to_csv(path_clones_merged)\n",
    "    print(\"clones_merged dataframe created and saved, continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d95a4-b985-4741-9aa1-b09a39a87000",
   "metadata": {},
   "source": [
    "---\n",
    "* Creating df with the relevent mutations infromation for each clone\n",
    "* Cleaning the df and adding relevent data\n",
    "* Saving the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c223f3-59ef-4410-9ddc-d267864f16d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85841/85841 [00:22<00:00, 3775.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covid_vaccine_new.mut_df dataframe created and saved, continuing....\n"
     ]
    }
   ],
   "source": [
    "path_mut_df = \"tables_processed\\\\{db}\\\\{db}.mut_df.csv\".format(db=mysql_db)\n",
    "\n",
    "if os.path.exists(path_mut_df):\n",
    "    mut_df = pd.read_csv(path_mut_df,index_col=0)\n",
    "    print(\"mut_df dataframe exists, loading and continuing....\")\n",
    "\n",
    "else:           \n",
    "    clones_merged[\"regions_all\"] = clones_merged[\"mutations\"].apply(mut_regall)\n",
    "    clones_raval = clones_merged.copy()\n",
    "    ra_val = []\n",
    "\n",
    "    with tqdm(range(0,len(clones_raval))) as tqdm_reval:\n",
    "        for i in tqdm_reval:\n",
    "            length = len(clones_raval.loc[i,\"regions_all\"]) # length of the list, number of mutations is the colum\n",
    "            value = clones_raval.loc[i,\"regions_all\"] # the value mutations themselves list of lists/ list / np.nan\n",
    "            id_value = clones_raval.loc[i,\"id\"] # id value of the row\n",
    "            clone_id = clones_raval.loc[i,\"clone_id\"] # clone_id value of the row\n",
    "            subject_id = clones_raval.loc[i,\"subject_id\"]# subject_id value of the row\n",
    "            sample_id = clones_raval.loc[i,\"sample_id\"] # sample_id value of the row\n",
    "            funct = clones_raval.loc[i,\"functional\"] # functional value of the clone\n",
    "            total_cnt = clones_raval.loc[i,\"total_cnt\"] # target of the antibody\n",
    "            unique_cnt = clones_raval.loc[i,\"unique_cnt\"] # unique sequence is clone\n",
    "            germline = clones_raval.loc[i,\"germline\"] #germline sequence\n",
    "            top_seq = clones_raval.loc[i,\"top_copy_seq_sequence\"] #top copy of sequence\n",
    "            metadata = clones_raval.loc[i,metadata_df.columns[1:]].values.flatten().tolist() #metadata list value\n",
    "            ins_val = [id_value, clone_id, subject_id, sample_id, funct, total_cnt,unique_cnt, germline, top_seq] + metadata\n",
    "            \n",
    "            # if single row of mutation\n",
    "            if length == 1:\n",
    "                to_aas = value[0][4].replace(\" \",\"\").replace(\"''\",\"\").split(\",\")\n",
    "                \n",
    "                if (len(to_aas) == 1):\n",
    "                    temp_list = list(value[0])\n",
    "                    ra_val.append(ins_val + temp_list) \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(0,len(to_aas)):\n",
    "                        temp_list = list(value[0])\n",
    "                        temp_list[4] = to_aas[i]\n",
    "                        ra_val.append(ins_val + temp_list)\n",
    "            \n",
    "            # if multiple rows of mutations\n",
    "            if length > 1:\n",
    "                for j in range(0,length):\n",
    "                        sub_value = list(value[j]) #each row\n",
    "                        temp_list = sub_value\n",
    "                        \n",
    "                        # making sure that the length of the list is correct, in some rows there is missing values\n",
    "                        if len(sub_value) == 8:\n",
    "                            to_aas = sub_value[4].replace(\" \",\"\").replace(\"'\",\"\").split(\",\")\n",
    "                            \n",
    "                            if len(to_aas) == 1:\n",
    "                                ra_val.append(ins_val + temp_list)\n",
    "                            elif len(to_aas) > 1:\n",
    "                                for aa in set(to_aas): # set() removes duplicate values\n",
    "                                    temp_list[4] = aa\n",
    "                                    ra_val.append(ins_val + temp_list)\n",
    "                                             \n",
    "            elif length == 0:\n",
    "                ra_val.append(ins_val + np.full(shape=len(value), fill_value=np.nan).tolist())\n",
    "        \n",
    "        mut_df_cols = [\"id\", \"clone_id\", \"subject_id\", \"sample_id\", \"functional\", \"total_cnt\",\"unique_cnt\", \"germline\", \"top_seq\"]\n",
    "        mut_info_cols = [\"pos\",\"from_nt\",\"from_aa\",\"to_nt\",\"to_aas\",\"unique\",\"total\",\"intermidiate_aa\"]\n",
    "        \n",
    "        mut_df = pd.DataFrame(data=ra_val, columns = mut_df_cols + metadata_df.columns[1:].tolist() + mut_info_cols)\n",
    "        mut_df[\"to_aas\"] = mut_df[\"to_aas\"].str.replace(\"'\",\"\") #cleaning to_aas string\n",
    "        mut_df.replace({\"to_aas\":{\"None\":np.nan}}, inplace=True) #turining none values to np.nan\n",
    "        mut_df.dropna(axis=0,subset=[\"pos\",\"to_aas\"], ignore_index=True, inplace=True) #dropping null rows of \"pos\" and \"to_aas\"\n",
    "        \n",
    "        mut_df.insert(6,\"pos_aa\",np.nan) #inserting amino acid position column\n",
    "        mut_df.insert(6,\"pos_nt\",np.nan) #inserting nucleotide position column\n",
    "        mut_df.loc[:,\"pos_nt\"] = mut_df.loc[:,\"pos\"].apply(int)+1 #filling the pos_nt column\n",
    "        mut_df.loc[:,\"pos_aa\"] = ((mut_df.loc[:,\"pos_nt\"])/3).apply(round_up) #fillint the pos_aa column \n",
    "        mut_df.astype({\"pos_nt\":\"int\",\"pos_aa\":\"int\"})\n",
    "        mut_df.drop(axis=1,columns=\"pos\",inplace=True) #dropping the og column (it was -1 in position...)\n",
    "        mut_df[\"syn\"] = (mut_df[\"from_aa\"] == mut_df[\"to_aas\"]).apply(int) #creating syn column\n",
    "    \n",
    "        mut_df.to_csv(path_mut_df)\n",
    "        print(\"{}.mut_df dataframe created and saved, continuing....\".format(mysql_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459e489-9c65-45a2-b050-65c7e0e3c6fb",
   "metadata": {},
   "source": [
    "---\n",
    "* Importing amino acid type dictionary.\n",
    "* Creating function that will cat string of position.before_aa_type.after_aa_type.\n",
    "* Assigining output to new colum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aeb8618-f7b9-49ee-93c8-f67fa6d803b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"scripts\\\\aa_type_dict.py\"\n",
    "\n",
    "with open(path, \"r\") as type_raw:\n",
    "    type_str = type_raw.read()\n",
    "    type_dict = json.loads(type_str)\n",
    "    type_raw.close()\n",
    "\n",
    "type_dict_T = {vi: k  for k, v in type_dict.items() for vi in v}\n",
    "\n",
    "get_change = (lambda df,idict,paa,faa,taa: \".\".join([str(int(df[paa])), idict[df[faa]], idict[df[taa]]]))\n",
    "\n",
    "mut_df[\"from_type\"] = mut_df[\"from_aa\"].apply((lambda x,idict: idict[x]), idict=type_dict_T)\n",
    "mut_df[\"to_type\"] = mut_df[\"to_aas\"].apply((lambda x,idict: idict[x]), idict=type_dict_T)\n",
    "mut_df[\"type_change\"] =  mut_df.apply(get_change, \n",
    "                                      idict = type_dict_T,\n",
    "                                      paa = \"pos_aa\",\n",
    "                                      faa = \"from_aa\",\n",
    "                                      taa = \"to_aas\",\n",
    "                                      axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dbcad-70f5-4a52-941e-af743d3a3e17",
   "metadata": {},
   "source": [
    "---\n",
    "* Filter unwated mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3745163e-9465-49c2-8e9b-041630d31876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Filtred mutations rows, conitnuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_24156\\3471186527.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filt_mut_df[\"subject_id\"] = filt_mut_df[\"subject_id\"].apply(str)\n"
     ]
    }
   ],
   "source": [
    "filt_pos_aa = (mut_df[\"pos_aa\"].between(1,104,inclusive='both')) # from aa positions 1->104\n",
    "filt_functional = (mut_df[\"functional\"] == 1) # only functional clones\n",
    "filt_unique_cnt = (mut_df[\"unique_cnt\"] > 1) # only clones with more than 1 unique sequence\n",
    "filt_syn = (mut_df[\"syn\"] == 0) # only non-syn mutations (substitutions)\n",
    "\n",
    "filt_mut_df = mut_df[filt_pos_aa & filt_functional & filt_syn & filt_unique_cnt]\n",
    "\n",
    "if bool(strtobool(mysql_filter_trunk)):\n",
    "    filt_mut_df[\"mut_freq\"] = filt_mut_df[\"unique\"]/filt_mut_df[\"unique_cnt\"]\n",
    "    filt_mut_df = filt_mut_df[(filt_mut_df[\"mut_freq\"] >= float(mysql_trunk_threshold)) & \n",
    "    (filt_mut_df[\"unique_cnt\"] > int(mysql_nclones_threshold))]\n",
    "    print(\"-> applied fitlers\")\n",
    "   \n",
    "filt_mut_df[\"subject_id\"] = filt_mut_df[\"subject_id\"].apply(str)\n",
    "\n",
    "print(\"-> Filtred mutations rows, conitnuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f645c2-96fc-48e8-86d7-92f32681fb06",
   "metadata": {},
   "source": [
    "---\n",
    "* Changing order of sorting if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4d7be4-d82a-4b5d-b966-1ca4c6541a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> change order: False \n",
      "-> Order: ['subject_id', 'ab_target', 'time_point']\n"
     ]
    }
   ],
   "source": [
    "# Changing the order of the label presentation\n",
    "change_order = bool(strtobool(mysql_order_change))\n",
    "\n",
    "catagories_dict = {}\n",
    "\n",
    "if change_order:\n",
    "    val_list = mysql_order_newval.split(\",\")\n",
    "    \n",
    "    for i in val_list:\n",
    "        temp_val_list = list(filt_mut_df[i].unique())\n",
    "        temp_val_list.sort()\n",
    "        catagories_dict[i] = temp_val_list\n",
    "\n",
    "else:\n",
    "    unique_subj = list(filt_mut_df[\"subject_id\"].unique())\n",
    "    \n",
    "    catagories_dict[\"subject_id\"] = unique_subj\n",
    "    val_list = list(metadata_df.columns[1:])\n",
    "    val_list.insert(0,\"subject_id\")\n",
    "    \n",
    "    for i in val_list:\n",
    "        catagories_dict[i] = np.sort(filt_mut_df[i].unique())  \n",
    "\n",
    "print(\"-> change order: {}\".format(change_order), \"\\n-> Order: {}\".format(list(catagories_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20a700-8bf3-4e2f-9019-3b4f8bdc394a",
   "metadata": {},
   "source": [
    "---\n",
    "* Remapping values if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e78288-6760-4672-aa6e-8f1c8c95bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values for metadata remap\n",
    "remap_subject_id = [\"subj_\"+i for i in np.sort(filt_mut_df[\"subject_id\"].unique())]\n",
    "remap_time_point = [i[0] for i in np.sort(filt_mut_df[\"time_point\"].unique())]\n",
    "remap_ab_type = [\"sp\" if i == \"Spike+ Mem B\" else \"sn\" for i in np.sort(filt_mut_df[\"ab_target\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0201c763-ce21-4a86-90b7-2f93fa99ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remap = bool(strtobool(mysql_remap))\n",
    "\n",
    "if remap:\n",
    "    custom_list = [remap_subject_id, \n",
    "                   remap_time_point,\n",
    "                   remap_ab_type]\n",
    "\n",
    "    remap_vals = {old_val:new_val for old_dic,new_dic in zip(catagories_dict.values(),custom_list) for old_val,new_val in zip(old_dic,new_dic)}\n",
    "    catagories_dict = {i:j for i,j in zip(list(catagories_dict.keys()),custom_list)}\n",
    "    \n",
    "   \n",
    "    filt_mut_df.loc[:,val_list] = filt_mut_df.loc[:,val_list].replace(remap_vals)\n",
    "    catagories_dict = {i:j for i,j in zip(list(catagories_dict.keys()),[i for i in catagories_dict.values()])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a840b-b9f6-4216-bf55-5d4aa4c7cc57",
   "metadata": {},
   "source": [
    "---\n",
    "* Creating & saving the metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99ba249-62ae-4893-a785-8a57156255c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved metadata matrix.\n"
     ]
    }
   ],
   "source": [
    "cond_matrix = metadata_cond(catagories_dict).astype(\"str\")\n",
    "cond_matrix.to_csv(\"tables_processed\\\\{db}\\\\{db}.metadata_matrix.csv\".format(db=config[\"mysql\"][\"db\"]))\n",
    "\n",
    "print(\"-> Saved metadata matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa86e8f-fe22-4868-ac6d-e020e7ac8a8d",
   "metadata": {},
   "source": [
    "---\n",
    "[https://www.imgt.org/IMGTScientificChart/Numbering/IMGTIGVLsuperfamily.html](https://www.imgt.org/IMGTScientificChart/Numbering/IMGTIGVLsuperfamily.html)\n",
    "\n",
    "* Choosing the analysis aa positions range.\n",
    "* Creating frequencies of subtition mutations for LPA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6cb76e-3b7b-4580-ba66-a8a30dbbcdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> documents frequencies saved. \n",
      "-> Domain frequenceis saved.\n"
     ]
    }
   ],
   "source": [
    "range_all = range(1,105) #FR1, CDR1, FR2, CDR2, FR3\n",
    "#range_cdr = list(range(27,39)) + list(range(56,66)) #CDR1, CDR2\n",
    "#range_fr = list(range(1,27))+list(range(39,56))+list(range(66,105)) #FR1,FR2,FR3\n",
    "\n",
    "df_input = filt_mut_df.copy()\n",
    "pos_range = range_all\n",
    "output_summerylis = []\n",
    "df_output = pd.DataFrame({\"pos_aa\":pos_range})\n",
    "\n",
    "for i in tqdm(range(0,len(cond_matrix))):\n",
    "    df_temp = df_input.copy()\n",
    "    temp_vals = cond_matrix.iloc[i,:].values\n",
    "\n",
    "    for name, val in zip(val_list,temp_vals): \n",
    "        df_temp = df_temp[df_temp[name] == val]\n",
    "\n",
    "    col_name = \".\".join(temp_vals)\n",
    "    output_summerylis.append([col_name,len(df_temp[\"clone_id\"].unique())])\n",
    "    \n",
    "    df_getoutput = get_output(df_temp,col_name,pos_range,method=\"nunique_count\")\n",
    "    df_output = df_output.merge(df_getoutput, on=\"pos_aa\", how=\"left\")\n",
    "\n",
    "if os.path.exists(\"tables_processed\\\\{db}\".format(db=mysql_db)) == False:\n",
    "    os.mkdir(\"tables_processed\\\\{db}\".format(db=mysql_db))\n",
    "\n",
    "df_output.to_csv(\"tables_processed\\\\{db}\\\\{db}.{val}-documents_substitution_tfreq.csv\".format(db=mysql_db,val=\".\".join(val_list)))\n",
    "\n",
    "df_domain = get_output(df_input,\"domain\",pos_range,method=\"nunique_weight\")\n",
    "df_domain.to_csv(\"tables_processed\\\\{db}\\\\{db}.{val}-domain_substitution_tfreq.csv\".format(db=mysql_db,val=\".\".join(val_list)))\n",
    "\n",
    "print(\"-> documents frequencies saved.\",\"\\n-> Domain frequenceis saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1460ec-ce4a-4705-b53f-ab06a783c801",
   "metadata": {},
   "source": [
    "---\n",
    "* Creating domain output file.\n",
    "* Creating documents output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ceb4304-b53d-40ce-bb7b-bc21a3ccbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 2210.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> output files saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_output = \"tables_output\\\\{}\".format(mysql_db)\n",
    "if os.path.exists(path_output) == False:\n",
    "    os.mkdir(path_output)\n",
    "    print(\"-> {} folder created.\".format(path_output))\n",
    "\n",
    "output_domain = df_domain.rename({\"pos_aa\":\"element\", \"domain\":\"global_weight\"},axis=1)\n",
    "\n",
    "for i in tqdm(df_output.columns[1:]):\n",
    "    document = i\n",
    "    elements =  df_output[\"pos_aa\"]\n",
    "    frequency = df_output[i]\n",
    "    temp_df = pd.DataFrame({\"document\":i, \n",
    "                            \"element\":elements, \n",
    "                            \"frequency_in_document\":frequency})\n",
    "\n",
    "    if i == df_output.columns[1]:\n",
    "        output_documents = temp_df\n",
    "    else:\n",
    "        output_documents = pd.concat([output_documents, temp_df])\n",
    "\n",
    "for i,j in zip([output_domain, output_documents],[\"domain\",\"documents\"]):\n",
    "    i.to_csv(path_output+\"\\\\{db}.{type}.csv\".format(db=mysql_db, type=j))\n",
    "\n",
    "print(\"-> output files saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739080e-4d6b-4989-891b-46c4171e89d5",
   "metadata": {},
   "source": [
    "---\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e49e6a-b03e-4ebc-ae29-f7ec8b9fa2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:05<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "path_output = \"tables_output\\\\{}\".format(mysql_db)\n",
    "\n",
    "for i in tqdm(range(0,len(cond_matrix))):\n",
    "    df_temp = df_input.copy()\n",
    "    temp_vals = cond_matrix.iloc[i,:].values\n",
    "\n",
    "    for name, val in zip(val_list,temp_vals): \n",
    "        df_temp = df_temp[df_temp[name] == val]\n",
    "\n",
    "    col_name = \".\".join(temp_vals)\n",
    "\n",
    "    out_df = df_temp.groupby(\"type_change\").agg({\"clone_id\":\"nunique\"}).rename({\"clone_id\":\"freq\"},axis=1).reset_index()\n",
    "    temp_freq = pd.DataFrame({\"document\":col_name, \"element\":out_df[\"type_change\"], \"frequency_in_document\":out_df[\"freq\"]})\n",
    "    temp_freq[\"pos\"] = temp_freq[\"element\"].apply(lambda x: int(x.split(\".\")[0]))\n",
    "    temp_freq = temp_freq.sort_values(\"pos\").reset_index(drop=True)\n",
    "    temp_freq = temp_freq.iloc[:,0:-1]\n",
    "\n",
    "    if i == 0:\n",
    "        changetype_df = temp_freq\n",
    "\n",
    "    else:\n",
    "        changetype_df = pd.concat([changetype_df, temp_freq], axis=0)\n",
    "\n",
    "    changetype_df.to_csv(path_output+\"\\\\{db}.{type}.csv\".format(db=mysql_db, type=\"change_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07e676e2-312c-4cf0-8365-5a44678e1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input[\"dataset\"] = df_input[\"ab_target\"] + \".\" + df_input[\"time_point\"] + \".\" + df_input[\"subject_id\"]\n",
    "#df_input[\"dataset\"] = df_input[\"subject_id\"] + \".\" +df_input[\"ab_target\"]\n",
    "\n",
    "sizes= pd.DataFrame(df_input.groupby([\"dataset\",\"pos_aa\"]).size().reset_index())\n",
    "sizes.rename(columns={0:\"n_mut\"},inplace=True)\n",
    "sizes.to_csv(path_output+\"\\\\{db}.{type}.csv\".format(db=mysql_db, type=\"dataset_sizes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619c943",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09acd1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>element</th>\n",
       "      <th>frequency_in_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subj_3.1.sn</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subj_3.1.sn</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subj_3.1.sn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subj_3.1.sn</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subj_3.1.sn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>subj_7.4.sp</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>subj_7.4.sp</td>\n",
       "      <td>101</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>subj_7.4.sp</td>\n",
       "      <td>102</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>subj_7.4.sp</td>\n",
       "      <td>103</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>subj_7.4.sp</td>\n",
       "      <td>104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        document  element  frequency_in_document\n",
       "0    subj_3.1.sn        1                    0.0\n",
       "1    subj_3.1.sn        2                    0.0\n",
       "2    subj_3.1.sn        3                    0.0\n",
       "3    subj_3.1.sn        4                    0.0\n",
       "4    subj_3.1.sn        5                    0.0\n",
       "..           ...      ...                    ...\n",
       "99   subj_7.4.sp      100                    0.0\n",
       "100  subj_7.4.sp      101                    0.0\n",
       "101  subj_7.4.sp      102                    0.0\n",
       "102  subj_7.4.sp      103                    0.0\n",
       "103  subj_7.4.sp      104                    0.0\n",
       "\n",
       "[3120 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
